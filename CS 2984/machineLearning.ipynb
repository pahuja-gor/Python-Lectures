{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "target\n",
      "target_names\n",
      "DESCR\n",
      "feature_names\n",
      "filename\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# load the iris dataset and print out the keys from the \n",
    "# corresponding dictionary\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "for key in iris.keys():\n",
    "    print(key)\n",
    "    \n",
    "#print(iris['data'])\n",
    "#print(iris['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a decision tree classifier and train it on all data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(iris['data'], iris['target'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      1.00      1.00        50\n",
      "           2       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00       150\n",
      "   macro avg       1.00      1.00      1.00       150\n",
      "weighted avg       1.00      1.00      1.00       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "actual = iris['target']\n",
    "predicted = model.predict(iris['data'])\n",
    "\n",
    "# Create a classification report and a confusion matrix\n",
    "\n",
    "print(metrics.classification_report(actual, predicted))\n",
    "print(metrics.confusion_matrix(actual, predicted))\n",
    "\n",
    "# Why is our output perfect?  Because we trained on everything! It\n",
    "# knows all of the answers already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n",
      "(120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Throwing the input into a DataFrame for fun\n",
    "irisDF = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "#print(irisDF)\n",
    "\n",
    "# train_test_split divides our data into training and testing groups\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(irisDF, iris['target'], test_size=0.2)\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.80      0.89      0.84         9\n",
      "           2       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.89        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "[[11  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  2  8]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "actual = y_test\n",
    "predicted = model.predict(x_test)\n",
    "\n",
    "# Now we have other model results based off of real predictions, but\n",
    "# there's still a risk that there is bias in the data\n",
    "print(metrics.classification_report(actual, predicted))\n",
    "print(metrics.confusion_matrix(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96 1.   0.92 0.92 0.88 1.  ]\n",
      "0.9466666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.94      0.94      0.94        50\n",
      "           2       0.94      0.94      0.94        50\n",
      "\n",
      "    accuracy                           0.96       150\n",
      "   macro avg       0.96      0.96      0.96       150\n",
      "weighted avg       0.96      0.96      0.96       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  3 47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "# cross-validation lets us use every item as a testing item by\n",
    "# running multiple passes through the data\n",
    "scores = cross_val_score(model, irisDF, iris['target'], cv=6)\n",
    "print(scores)\n",
    "\n",
    "sum = 0\n",
    "for score in scores:\n",
    "    sum += score\n",
    "print(sum/6)\n",
    "\n",
    "# we can also get the corresponding predictions from the model output\n",
    "predicted = cross_val_predict(model, irisDF, iris['target'], cv=5)\n",
    "actual = iris['target']\n",
    "\n",
    "print(metrics.classification_report(actual, predicted))\n",
    "print(metrics.confusion_matrix(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 50\n",
      "0 62\n",
      "2 38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# loading and fitting k-means\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(irisDF)\n",
    "\n",
    "# dictionary counting pattern to figure out how many flowers are\n",
    "# in each cluster\n",
    "freq = {}\n",
    "for label in model.labels_:\n",
    "    if label in freq.keys():\n",
    "        freq[label] += 1\n",
    "    else:\n",
    "        freq[label] = 1\n",
    "        \n",
    "for key in freq.keys():\n",
    "    print(key, freq[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n",
      "(60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load and prepare the MNIST dataset.  Convert the samples from\n",
    "# integers to floating-point [0,1] values\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.5671199 ,  0.18190444,  0.23910058, -0.43286905,  0.22094427,\n",
       "        -0.01396617, -0.00165793,  0.14976072,  0.79003084,  0.5319262 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Sequential model by stacking layers\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "\n",
    "# Run the first row on the untrained(!) model.  The output is a\n",
    "# \"logit\" or \"log-odd\" score, one for each class.  The softmax\n",
    "# function commented out converts that into a probability\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "#predictions = tf.nn.softmax(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.497569"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to create a loss function to penalize the neural network\n",
    "# training when it gets things wrong\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 34s 565us/sample - loss: 0.2923 - accuracy: 0.9158\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 30s 505us/sample - loss: 0.1413 - accuracy: 0.9578\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 27s 456us/sample - loss: 0.1058 - accuracy: 0.9684\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 37s 612us/sample - loss: 0.0857 - accuracy: 0.9737\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 36s 602us/sample - loss: 0.0718 - accuracy: 0.9775- loss: 0.0 - ETA: 0s - loss: 0.0718 - accuracy:  - ETA: 0s - loss: 0.0719 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dec44dcc08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now we're actually training our neural network!\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 3s - loss: 0.0735 - accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0734720635201782, 0.9776]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's the evaluation on the 10000 testing rows\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[1.4860373e-07, 4.2159733e-09, 1.9513641e-06, 1.5949721e-04,\n",
       "        4.7938536e-13, 1.4558890e-07, 1.4161849e-13, 9.9983633e-01,\n",
       "        4.0216290e-07, 1.5880281e-06],\n",
       "       [1.0570799e-09, 2.6996585e-03, 9.9721485e-01, 7.8138182e-05,\n",
       "        7.4671535e-13, 4.5263660e-06, 3.4815821e-07, 7.0389698e-14,\n",
       "        2.4514325e-06, 6.5205398e-12],\n",
       "       [2.4273345e-07, 9.9876130e-01, 1.8059496e-04, 1.5558811e-05,\n",
       "        5.9933529e-05, 2.2573875e-06, 6.9919970e-06, 8.9649140e-04,\n",
       "        7.5418488e-05, 1.1398979e-06],\n",
       "       [9.9991524e-01, 1.6450050e-10, 2.5753299e-05, 8.7456719e-08,\n",
       "        1.4571569e-07, 3.2265541e-07, 2.1557994e-06, 4.6026148e-06,\n",
       "        1.7943182e-06, 4.9858842e-05],\n",
       "       [1.3199647e-06, 1.0714654e-09, 2.1574242e-06, 1.4878186e-08,\n",
       "        9.9883062e-01, 9.2542173e-08, 5.6267220e-07, 1.2629027e-04,\n",
       "        4.5192987e-08, 1.0389428e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we wrap a Softmax layer into the Sequential model and \n",
    "# display the probabilities associated with each of the 10 digit\n",
    "# possibilities for the first 5 rows\n",
    "\n",
    "probability_model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "probability_model(x_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
